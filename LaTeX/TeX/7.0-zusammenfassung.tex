\section{Zusammenfassung}
\label{sec:zusammenfassung}

Software-Agenten sind maßgeblich an der Entwicklung des Internets beteiligt.
Die großen Mengen an Informationen, die sich durch das Internet bewegen, sind
lange schon viel zu groß, um alleinig von Menschen verarbeitet zu werden. Immer
schnellere und intelligentere Bots übernehmen inzwischen ein breites Spektrum
an Aufgaben. Diese Aufgaben können Schaden verursachen, wie Spam- oder
DDoS-Bots, oder auch versuchen, diesen zu unterbinden, wie Webcrawler, die das
Web auf Phishing-Seiten untersuchen.

Durch den rapiden Fortschritt im Bereich der Künstlichen Intelligenz werden
Technologien wie CAPTCHAs und Honeypots immer unzuverlässiger. Neue Ansätze,
Bots aus gewissen Bereichen des Internets auszusperren, wie der \emph{Zwang zur
Beglaubigung als Mensch}, sind umständlich und verlangen die Übergabe
persönlicher Daten an Dritte.

Spam-Nachrichten sind immer noch effektiv genug, um ein lukratives Geschäft zu
sein. Wie ein biologisches Virus verändern sie ihre Form, um Spam-Filtern zu
entwischen. Der beste Schutz gegen Spam ist daher, die eigene Adresse nicht
öffentlich zugänglich zu machen. Brute-Force-Bots versuchen Zugriff auf
Webseiten zu erlangen, indem sie Listen von häufigen Benutzernamen und
Passwörtern über Tage hinweg ausprobieren. Starke Passwörter sind ein Muss, um
im Internet sicher zu bleiben.

Projekte wie WWWEB beweisen, dass sich mit 250 Zeilen Code bereits mächtige
Software-Agenten programmieren lassen, die das Web nach Informationen
durchstöbern. Ein einzelner Laptop konnte so in drei Stunden 2.190
unterschiedliche Webseiten analysieren. Skaliert man diese Zahlen auf Tausende
von Hochleistungscomputer, bekommt man eine Vorstellung, wozu moderne
Software-Agenten in der Lage sind.
